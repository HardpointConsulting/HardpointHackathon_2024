{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **WEB SCRAPPING BASED AUTHOR AND TITILE EXTRACTION AND ALTERNATIVE TITLE GENERATION SYSTEM**"
      ],
      "metadata": {
        "id": "wHa60VHJG4Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries and data extraction\n",
        "!apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAxiYilMHXrg",
        "outputId": "42c81e16-c6ac-481c-dbd7-6fd865577963"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 0s (15.2 MB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    # Open the image file\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    img_gray = img.convert('L')\n",
        "\n",
        "    # Convert PIL image to OpenCV format\n",
        "    img_cv = np.array(img_gray)\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    img_blur = cv2.GaussianBlur(img_cv, (5, 5), 0)\n",
        "\n",
        "    return img_blur\n",
        "\n",
        "# Function to extract text and font size\n",
        "def extract_text_and_font_size(image_path):\n",
        "    # Preprocess the image\n",
        "    preprocessed_img = preprocess_image(image_path)\n",
        "\n",
        "    # Use Tesseract OCR to extract text\n",
        "    extracted_text = pytesseract.image_to_string(preprocessed_img)\n",
        "\n",
        "    # Get the bounding boxes and confidences of the recognized text\n",
        "    boxes = pytesseract.image_to_boxes(preprocessed_img)\n",
        "    font_sizes = []\n",
        "\n",
        "    # Parse bounding boxes to get font sizes\n",
        "    for box in boxes.splitlines():\n",
        "        _, _, _, _, font_size, *_ = box.split()\n",
        "        font_sizes.append(int(font_size))\n",
        "\n",
        "    # Return extracted text and font sizes\n",
        "    return extracted_text, font_sizes\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Path to the image file\n",
        "    image_path = '/content/doc_000016.png'\n",
        "\n",
        "    # Extract text and font sizes from the image\n",
        "    extracted_text, font_sizes = extract_text_and_font_size(image_path)\n",
        "\n",
        "    # Print the extracted text\n",
        "    print(\"Extracted Text:\")\n",
        "    print(extracted_text)\n",
        "\n",
        "    # Print the font sizes\n",
        "    print(\"\\nFont Sizes:\")\n",
        "    print(font_sizes)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ_-8ZjPXnog",
        "outputId": "836a5888-6ff1-49a4-d0ce-bfac09a01b30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            "The Contribution of Tobacco\n",
            "Constituents to Phenol Yield ,..“\n",
            "\n",
            ". of Cigarettes’\n",
            "a\n",
            "\n",
            "4. H. Bell, A. ©. Sounders and A. W. Speers\n",
            "Research Division, P. Lorillard Company, Inc.\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "Greensboro, North Carolina, U.S.A.\n",
            "‘\n",
            "yroiytic studies may be misleading. combustion tube at « fixed rate. All\n",
            "Moreover, when individual com- temperatures were controlled with-\n",
            "Pounds are prrolyted, the absence in IC. The trapping system com:\n",
            "‘SC ether tabneco components cannot sisted of a conventional cold trap\n",
            "‘e heglected in reaching conclusions, submerged. ina ry keeacetone,\n",
            "Since they may produce catalytic slurry. Phenol analyses were?\n",
            "fects carried out by the method described\n",
            "‘The addition of ©% labeled com- by Spears (9), except the. steam\n",
            "pounds to tobacco appears te offer distillation was emitted. The relax\n",
            "the most wnequivecal technique for tive standard deviation of the prro-\n",
            "salves Ge eautetn of pe lytic procedure was found to be\n",
            "Carers to smake conatiteret 19%.\n",
            "Licked compounds cam be added to\n",
            "thee ie ceiremaly small gun. Treer\n",
            "tities and no dificwlty 's experienced radionctivity was measured\n",
            "‘not imponsible. to ‘control. Adsi- in abiaining clearettes with normal jy Nigutd“acititation’ tesetie,\n",
            "ion of siznitcant amouats of ma. burwing chatacteriaticn Wh cotaetus tore pheoed Meth toni\n",
            "terial—Aive to tem percent by weight In this ivestiantion of phenol pre- Aj\\cr'tre srimiliatiee soticg one\n",
            "—is usually necessary to bring about cursors the pyrolytic method Wat Dyed of toluene “containing €\n",
            "demenctratic changes \\n smoke com niopted to develop preliminary con”\n",
            "{esition, and. wndoubtediy. the bure- chusions, and then the tracer tech.\n",
            "tng charecteriatics of the tobacco nique used to substantiate or refute\n",
            "are changed. Extraction of tobacco these conclusions =\n",
            "‘with sslvents to deplete certain con\n",
            "stituents produces similar alters: Experimenta! Methods ‘Aepeeaieataly four\n",
            "tions in the burning process AY Pyretytic tends wore tae one ht\n",
            "ne seein et The pyrolysis apparstos shown sn ethancl-water solution contain-\n",
            "by several investigators 'e sim in Figure I consisted of a 100 cm ing S.12 » 10° cpm of waiformly\n",
            "\n",
            "X25 em Vycor tube packed to labeled glucose [specific activity\n",
            "\n",
            "[\n",
            "I\n",
            "i\n",
            "i\n",
            ".\n",
            "i\n",
            "\n",
            "about 60 em with Vycor chips. The 200 me/mMt}. After thorough mix-\n",
            "\n",
            "  \n",
            " \n",
            "\n",
            "   \n",
            "      \n",
            "\n",
            "the yield of smoke components\n",
            "£2.10), bat only one investigation\n",
            "has dealt directly with the pre ironhilieed to ensure that al\n",
            "\n",
            "“ Tronbilited that all sal\n",
            "\n",
            "\"The\n",
            "\n",
            "‘unless the conditions which exist i\n",
            "\n",
            " \n",
            "\n",
            "(Todecem Selemee «49\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Font Sizes:\n",
            "[929, 929, 918, 938, 927, 918, 918, 924, 926, 928, 938, 917, 926, 917, 938, 917, 927, 926, 917, 928, 928, 917, 917, 917, 886, 886, 877, 877, 877, 883, 876, 883, 877, 877, 882, 876, 882, 876, 885, 886, 886, 876, 876, 886, 886, 884, 884, 885, 886, 880, 864, 868, 880, 833, 835, 845, 844, 844, 834, 844, 834, 834, 834, 841, 844, 834, 844, 837, 740, 736, 740, 736, 740, 740, 736, 740, 740, 740, 735, 735, 727, 740, 740, 740, 740, 735, 740, 740, 740, 740, 735, 740, 740, 734, 735, 726, 740, 740, 740, 735, 740, 740, 711, 711, 711, 711, 707, 711, 711, 711, 711, 711, 711, 711, 711, 707, 711, 711, 711, 711, 706, 711, 711, 711, 711, 706, 711, 711, 711, 711, 711, 711, 711, 706, 711, 711, 711, 697, 711, 711, 705, 711, 591, 354, 692, 692, 692, 692, 692, 692, 692, 692, 692, 692, 692, 701, 701, 701, 701, 701, 692, 692, 692, 692, 692, 692, 692, 692, 692, 701, 701, 701, 701, 701, 701, 636, 600, 600, 600, 600, 589, 600, 600, 600, 600, 600, 600, 600, 589, 600, 600, 600, 587, 600, 600, 589, 589, 589, 589, 589, 589, 589, 589, 589, 589, 589, 582, 589, 589, 589, 589, 589, 589, 589, 589, 589, 589, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 577, 577, 577, 577, 577, 577, 577, 577, 570, 577, 577, 577, 577, 577, 577, 577, 577, 577, 577, 577, 577, 577, 577, 574, 574, 574, 574, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 588, 588, 588, 588, 576, 576, 576, 576, 576, 576, 576, 576, 576, 576, 588, 588, 588, 588, 588, 575, 575, 575, 564, 575, 575, 575, 562, 575, 564, 564, 564, 564, 564, 564, 564, 564, 564, 564, 575, 575, 575, 575, 575, 575, 575, 575, 575, 575, 575, 563, 564, 564, 564, 575, 564, 575, 575, 575, 575, 575, 563, 575, 575, 575, 575, 575, 575, 563, 575, 575, 575, 575, 562, 575, 564, 552, 564, 564, 564, 552, 564, 564, 564, 564, 564, 564, 552, 564, 564, 552, 552, 552, 552, 551, 552, 552, 552, 552, 551, 564, 564, 564, 564, 564, 564, 564, 564, 564, 551, 564, 564, 564, 552, 549, 552, 552, 552, 552, 552, 551, 552, 552, 552, 552, 552, 551, 564, 564, 564, 564, 564, 564, 564, 564, 551, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 540, 539, 539, 539, 539, 539, 539, 539, 539, 539, 539, 551, 551, 551, 551, 551, 539, 539, 539, 539, 539, 539, 539, 539, 539, 539, 539, 538, 538, 525, 538, 538, 538, 538, 527, 538, 538, 525, 538, 538, 538, 538, 527, 538, 538, 538, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 527, 526, 526, 525, 526, 526, 520, 520, 516, 520, 520, 519, 519, 519, 519, 515, 519, 519, 519, 514, 519, 519, 515, 519, 515, 519, 519, 519, 519, 515, 519, 519, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 503, 515, 503, 503, 503, 503, 503, 503, 503, 503, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 515, 503, 515, 515, 515, 503, 515, 515, 515, 502, 515, 502, 515, 515, 515, 502, 515, 515, 515, 503, 515, 515, 515, 515, 515, 502, 515, 502, 502, 502, 491, 502, 502, 502, 490, 502, 502, 502, 502, 491, 502, 502, 502, 502, 502, 502, 488, 502, 502, 502, 490, 502, 502, 502, 491, 502, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 491, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 483, 483, 478, 483, 483, 478, 483, 483, 483, 483, 483, 483, 479, 483, 483, 483, 483, 483, 483, 483, 483, 483, 483, 479, 483, 483, 483, 483, 483, 479, 483, 483, 478, 483, 483, 483, 483, 483, 479, 483, 483, 483, 483, 483, 483, 483, 478, 483, 483, 483, 483, 483, 478, 483, 478, 483, 483, 483, 476, 483, 483, 465, 471, 471, 471, 467, 471, 471, 466, 471, 466, 471, 471, 471, 466, 471, 471, 466, 471, 464, 471, 471, 467, 471, 466, 471, 471, 471, 466, 471, 471, 471, 471, 464, 471, 464, 471, 471, 471, 471, 466, 471, 471, 465, 471, 466, 466, 466, 466, 452, 466, 466, 466, 453, 466, 466, 454, 466, 466, 466, 466, 453, 466, 466, 466, 466, 466, 466, 454, 466, 454, 454, 454, 448, 453, 453, 442, 453, 453, 453, 442, 442, 442, 442, 442, 442, 442, 442, 442, 453, 453, 453, 453, 453, 453, 453, 453, 453, 453, 453, 453, 441, 441, 441, 430, 441, 429, 430, 430, 430, 430, 430, 430, 430, 430, 430, 441, 441, 441, 441, 441, 441, 441, 441, 430, 441, 430, 441, 428, 441, 417, 417, 417, 417, 417, 417, 429, 429, 429, 429, 415, 418, 418, 418, 418, 418, 418, 418, 418, 418, 429, 429, 418, 418, 418, 418, 418, 418, 418, 418, 418, 418, 418, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 413, 425, 425, 425, 413, 413, 413, 413, 413, 413, 413, 413, 418, 418, 406, 418, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 418, 406, 406, 406, 406, 406, 406, 406, 406, 406, 406, 407, 407, 407, 407, 407, 418, 406, 418, 418, 418, 418, 406, 418, 418, 418, 418, 418, 418, 418, 418, 406, 418, 418, 418, 418, 405, 418, 418, 406, 418, 418, 418, 418, 406, 418, 418, 418, 402, 414, 414, 414, 401, 414, 414, 414, 414, 414, 401, 414, 414, 401, 414, 414, 414, 414, 401, 414, 401, 401, 401, 401, 401, 401, 401, 395, 407, 395, 407, 407, 395, 395, 395, 395, 395, 395, 395, 395, 395, 395, 395, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 394, 407, 407, 407, 407, 407, 407, 407, 407, 407, 407, 394, 407, 407, 407, 407, 407, 407, 389, 389, 405, 387, 405, 405, 405, 389, 405, 405, 405, 405, 387, 405, 405, 405, 405, 389, 405, 405, 389, 405, 389, 389, 388, 386, 405, 405, 383, 383, 383, 383, 383, 383, 383, 383, 383, 383, 382, 394, 394, 394, 394, 394, 383, 383, 383, 383, 383, 383, 383, 394, 394, 383, 383, 383, 383, 383, 383, 382, 394, 394, 394, 382, 394, 382, 382, 381, 382, 382, 382, 382, 382, 382, 382, 382, 382, 394, 394, 394, 394, 394, 394, 394, 394, 394, 394, 394, 394, 394, 378, 394, 394, 378, 394, 394, 394, 377, 394, 375, 394, 394, 377, 394, 394, 394, 394, 377, 394, 394, 394, 394, 377, 394, 394, 394, 394, 375, 394, 379, 371, 379, 379, 379, 379, 379, 371, 379, 379, 371, 371, 371, 371, 369, 371, 371, 371, 371, 379, 379, 379, 379, 379, 379, 379, 379, 379, 379, 379, 379, 379, 379, 379, 368, 379, 379, 379, 379, 379, 371, 371, 371, 371, 371, 371, 371, 371, 371, 370, 379, 379, 379, 379, 379, 379, 379, 379, 379, 366, 366, 366, 366, 366, 366, 365, 365, 365, 365, 365, 363, 365, 365, 365, 365, 365, 364, 365, 365, 365, 365, 365, 365, 365, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 359, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 370, 359, 370, 370, 370, 370, 358, 370, 370, 370, 370, 359, 370, 370, 359, 359, 359, 359, 359, 359, 358, 359, 359, 359, 356, 370, 370, 370, 370, 358, 358, 358, 358, 348, 358, 358, 358, 358, 358, 358, 347, 358, 348, 348, 348, 348, 348, 347, 348, 348, 348, 348, 348, 347, 358, 358, 358, 358, 358, 358, 358, 358, 358, 358, 358, 358, 347, 358, 358, 358, 358, 358, 358, 347, 358, 358, 346, 358, 358, 358, 346, 345, 358, 358, 358, 344, 358, 358, 358, 358, 346, 358, 340, 340, 335, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 336, 335, 340, 340, 340, 340, 340, 340, 340, 340, 340, 340, 340, 340, 340, 340, 340, 332, 340, 340, 340, 335, 340, 340, 334, 335, 335, 335, 335, 335, 335, 334, 335, 335, 335, 335, 334, 340, 340, 340, 340, 340, 340, 340, 340, 335, 322, 335, 335, 335, 335, 335, 324, 335, 335, 335, 324, 324, 324, 324, 324, 324, 324, 323, 324, 323, 335, 335, 324, 324, 324, 324, 324, 324, 324, 322, 335, 335, 322, 335, 335, 335, 335, 335, 335, 322, 335, 335, 335, 335, 335, 317, 323, 323, 323, 312, 323, 323, 323, 323, 323, 312, 323, 323, 323, 323, 310, 323, 323, 323, 311, 323, 323, 323, 323, 323, 323, 311, 323, 323, 323, 323, 323, 308, 308, 308, 308, 308, 308, 299, 308, 308, 308, 308, 308, 308, 299, 308, 308, 308, 308, 308, 308, 308, 299, 308, 308, 308, 308, 308, 308, 298, 308, 308, 308, 308, 308, 308, 308, 308, 308, 299, 308, 308, 308, 308, 308, 308, 308, 308, 298, 308, 308, 308, 306, 293, 306, 306, 291, 306, 290, 306, 306, 291, 306, 306, 306, 293, 306, 306, 290, 306, 299, 299, 299, 287, 299, 299, 287, 299, 299, 287, 299, 299, 299, 299, 287, 299, 299, 299, 299, 299, 299, 284, 299, 299, 286, 297, 297, 297, 297, 297, 286, 297, 297, 297, 297, 297, 281, 297, 297, 279, 297, 297, 278, 297, 279, 293, 297, 278, 290, 280, 293, 292, 276, 280, 275, 274, 275, 275, 274, 279, 279, 279, 270, 279, 279, 279, 279, 279, 269, 279, 279, 279, 279, 267, 279, 279, 267, 279, 266, 268, 279, 266, 279, 279, 268, 279, 279, 266, 267, 279, 279, 269, 279, 279, 279, 269, 279, 266, 279, 279, 279, 268, 279, 279, 269, 279, 267, 279, 279, 267, 279, 279, 279, 279, 268, 279, 279, 279, 268, 250, 268, 268, 268, 262, 268, 268, 268, 268, 268, 268, 268, 268, 262, 268, 268, 268, 268, 268, 268, 268, 268, 268, 247, 250, 250, 268, 256, 268, 268, 268, 257, 268, 268, 256, 257, 257, 257, 257, 256, 257, 257, 257, 256, 268, 268, 268, 268, 268, 268, 268, 268, 268, 268, 254, 256, 268, 256, 255, 268, 268, 256, 268, 253, 268, 253, 255, 255, 256, 256, 255, 256, 256, 256, 255, 256, 256, 243, 245, 245, 242, 242, 245, 245, 245, 242, 245, 243, 244, 244, 244, 244, 244, 244, 244, 244, 244, 243, 241, 245, 245, 245, 245, 245, 245, 245, 244, 244, 244, 244, 244, 241, 244, 244, 244, 244, 241, 244, 244, 244, 244, 244, 243, 242, 243, 243, 243, 243, 243, 243, 0, 0, 0, 0, 0, 0, 232, 232, 232, 232, 232, 241, 241, 241, 241, 232, 232, 232, 232, 232, 241, 241, 229, 241, 229, 241, 241, 232, 241, 241, 241, 241, 232, 241, 232, 241, 241, 230, 231, 230, 241, 232, 241, 232, 231, 232, 232, 232, 229, 230, 231, 231, 231, 231, 231, 231, 231, 231, 231, 231, 227, 220, 204, 200, 199, 200, 188, 182, 171, 183, 182, 183, 227, 227, 227, 227, 227, 227, 227, 227, 226, 226, 226, 226, 226, 226, 226, 224, 224, 224, 224, 224, 224, 224, 224, 224, 223, 213, 215, 215, 215, 215, 215, 213, 214, 214, 214, 214, 214, 214, 214, 211, 211, 211, 212, 212, 212, 212, 212, 212, 212, 212, 212, 212, 212, 212, 210, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 202, 201, 202, 201, 199, 201, 200, 201, 201, 200, 201, 201, 199, 199, 199, 194, 193, 194, 194, 194, 194, 194, 194, 193, 194, 194, 206, 206, 193, 193, 193, 193, 193, 193, 206, 206, 206, 206, 193, 193, 191, 196, 196, 196, 196, 196, 196, 196, 196, 196, 196, 195, 194, 195, 195, 194, 195, 195, 195, 195, 195, 195, 183, 183, 183, 183, 152, 152, 152, 152, 143, 152, 152, 152, 152, 142, 143, 143, 143, 142, 143, 143, 143, 143, 141, 141, 152, 152, 152, 152, 152, 152, 152, 152, 152, 139, 138, 131, 53, 56, 56, 56, 55, 56, 56, 53, 55, 55, 52, 55, 53, 55, 53, 51, 54, 54, 1000, 956]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_entities(text):\n",
        "    # Split the text by lines\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Initialize a list to store entities\n",
        "    entities = []\n",
        "\n",
        "    # Create a dictionary for each line with line number and content\n",
        "    for i, line in enumerate(lines):\n",
        "        line_entity = {\n",
        "            \"line_number\": i + 1,\n",
        "            \"content\": line.strip()\n",
        "        }\n",
        "        entities.append(line_entity)\n",
        "\n",
        "    return entities\n",
        "\n",
        "#\n",
        "\n",
        "entities = convert_to_entities(extracted_text)\n",
        "\n",
        "# Print entities\n",
        "for entity in entities:\n",
        "    print(entity)\n",
        "class Paper:\n",
        "    def __init__(self, title, authors, content):\n",
        "        self.title = title\n",
        "        self.authors = authors\n",
        "        self.content = content\n",
        "\n",
        "def extract_paper_info(entity_data):\n",
        "    title = \"\"\n",
        "    authors = \"\"\n",
        "    content = \"\"\n",
        "    for entity in entity_data:\n",
        "        if entity['line_number'] in [2, 3, 5]:\n",
        "            title += entity['content'] + \" \"\n",
        "        elif entity['line_number'] == 8:\n",
        "            authors = entity['content']\n",
        "        else:\n",
        "            content += entity['content'] + \"\\n\"\n",
        "    return Paper(title.strip(), authors.strip(), content.strip())\n",
        "\n",
        "# Example entity data (replace this with your actual entity data)\n",
        "\n",
        "# Extract information and create a Paper object\n",
        "# Extract information and create a Paper object\n",
        "paper = extract_paper_info(entities)\n",
        "\n",
        "# Access the extracted information\n",
        "print(\"Title:\", paper.title)\n",
        "print(\"Authors:\", paper.authors)\n",
        "print(\"Content:\", paper.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVrJSAK-aN42",
        "outputId": "6ea53b21-50a2-4a08-b646-7c7da99efd73"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'line_number': 1, 'content': ''}\n",
            "{'line_number': 2, 'content': 'The Contribution of Tobacco'}\n",
            "{'line_number': 3, 'content': 'Constituents to Phenol Yield ,..“'}\n",
            "{'line_number': 4, 'content': ''}\n",
            "{'line_number': 5, 'content': '. of Cigarettes’'}\n",
            "{'line_number': 6, 'content': 'a'}\n",
            "{'line_number': 7, 'content': ''}\n",
            "{'line_number': 8, 'content': '4. H. Bell, A. ©. Sounders and A. W. Speers'}\n",
            "{'line_number': 9, 'content': 'Research Division, P. Lorillard Company, Inc.'}\n",
            "{'line_number': 10, 'content': ''}\n",
            "{'line_number': 11, 'content': 'Greensboro, North Carolina, U.S.A.'}\n",
            "{'line_number': 12, 'content': 'yroiytic studies may be misleading. combustion tube at « fixed rate. All'}\n",
            "{'line_number': 13, 'content': 'Moreover, when individual com- temperatures were controlled with-'}\n",
            "{'line_number': 14, 'content': 'Pounds are prrolyted, the absence in IC. The trapping system com:'}\n",
            "{'line_number': 15, 'content': '‘SC ether tabneco components cannot sisted of a conventional cold trap'}\n",
            "{'line_number': 16, 'content': '‘e heglected in reaching conclusions, submerged. ina ry keeacetone,'}\n",
            "{'line_number': 17, 'content': 'Since they may produce catalytic slurry. Phenol analyses were?'}\n",
            "{'line_number': 18, 'content': 'fects carried out by the method described'}\n",
            "{'line_number': 19, 'content': '‘The addition of ©% labeled com- by Spears (9), except the. steam'}\n",
            "{'line_number': 20, 'content': 'pounds to tobacco appears te offer distillation was emitted. The relax'}\n",
            "{'line_number': 21, 'content': 'the most wnequivecal technique for tive standard deviation of the prro-'}\n",
            "{'line_number': 22, 'content': 'salves Ge eautetn of pe lytic procedure was found to be'}\n",
            "{'line_number': 23, 'content': 'Carers to smake conatiteret 19%.'}\n",
            "{'line_number': 24, 'content': 'Licked compounds cam be added to'}\n",
            "{'line_number': 25, 'content': 'thee ie ceiremaly small gun. Treer'}\n",
            "{'line_number': 26, 'content': \"tities and no dificwlty 's experienced radionctivity was measured\"}\n",
            "{'line_number': 27, 'content': '‘not imponsible. to ‘control. Adsi- in abiaining clearettes with normal jy Nigutd“acititation’ tesetie,'}\n",
            "{'line_number': 28, 'content': 'ion of siznitcant amouats of ma. burwing chatacteriaticn Wh cotaetus tore pheoed Meth toni'}\n",
            "{'line_number': 29, 'content': \"terial—Aive to tem percent by weight In this ivestiantion of phenol pre- Aj\\\\cr'tre srimiliatiee soticg one\"}\n",
            "{'line_number': 30, 'content': '—is usually necessary to bring about cursors the pyrolytic method Wat Dyed of toluene “containing €'}\n",
            "{'line_number': 31, 'content': 'demenctratic changes'}\n",
            "{'line_number': 32, 'content': 'smoke com niopted to develop preliminary con”'}\n",
            "{'line_number': 33, 'content': '{esions, and. wndoubtediy. the bure- chusions, and then the tracer tech.'}\n",
            "{'line_number': 34, 'content': 'tng charecteriatics of the tobacco nique used to substantiate or refute'}\n",
            "{'line_number': 35, 'content': 'are changed. Extraction of tobacco these conclusions ='}\n",
            "{'line_number': 36, 'content': ''}\n",
            "{'line_number': 37, 'content': 'with sslvents to deplete certain con'}\n",
            "{'line_number': 38, 'content': 'stituents produces similar alters: Experimenta! Methods ‘Aepeeaieataly four'}\n",
            "{'line_number': 39, 'content': 'tions in the burning process AY Pyretytic tends wore tae one ht'}\n",
            "{'line_number': 40, 'content': 'ne seein et The pyrolysis apparstos shown sn ethancl-water solution contain-'}\n",
            "{'line_number': 41, 'content': \"by several investigators 'e sim in Figure I consisted of a 100 cm ing S.12 » 10° cpm of waiformly\"}\n",
            "{'line_number': 42, 'content': ''}\n",
            "{'line_number': 43, 'content': 'X25 em Vycor tube packed to labeled glucose [specific activity'}\n",
            "{'line_number': 44, 'content': ''}\n",
            "{'line_number': 45, 'content': '['}\n",
            "{'line_number': 46, 'content': 'I'}\n",
            "{'line_number': 47, 'content': 'i'}\n",
            "{'line_number': 48, 'content': 'i'}\n",
            "{'line_number': 49, 'content': '.'}\n",
            "{'line_number': 50, 'content': 'i'}\n",
            "{'line_number': 51, 'content': ''}\n",
            "{'line_number': 52, 'content': 'about 60 em with Vycor chips. The 200 me/mMt}. After thorough mix-'}\n",
            "{'line_number': 53, 'content': ''}\n",
            "{'line_number': 54, 'content': ''}\n",
            "{'line_number': 55, 'content': ''}\n",
            "{'line_number': 56, 'content': ''}\n",
            "{'line_number': 57, 'content': ''}\n",
            "{'line_number': 58, 'content': 'the yield of smoke components'}\n",
            "{'line_number': 59, 'content': '£2.10), bat only one investigation'}\n",
            "{'line_number': 60, 'content': 'has dealt directly with the pre ironhilieed to ensure that al'}\n",
            "{'line_number': 61, 'content': ''}\n",
            "{'line_number': 62, 'content': '“ Tronbilited that all sal'}\n",
            "{'line_number': 63, 'content': ''}\n",
            "{'line_number': 64, 'content': '‘unless the conditions which exist i'}\n",
            "{'line_number': 65, 'content': ''}\n",
            "{'line_number': 66, 'content': ''}\n",
            "{'line_number': 67, 'content': ''}\n",
            "{'line_number': 68, 'content': '(Todecem Selemee «49'}\n",
            "{'line_number': 69, 'content': ''}\n",
            "Title: The Contribution of Tobacco Constituents to Phenol Yield ,..“ . of Cigarettes’\n",
            "Authors: 4. H. Bell, A. ©. Sounders and A. W. Speers\n",
            "Content: a\n",
            "\n",
            "Research Division, P. Lorillard Company, Inc.\n",
            "\n",
            "Greensboro, North Carolina, U.S.A.\n",
            "yroiytic studies may be misleading. combustion tube at « fixed rate. All\n",
            "Moreover, when individual com- temperatures were controlled with-\n",
            "Pounds are prrolyted, the absence in IC. The trapping system com:\n",
            "‘SC ether tabneco components cannot sisted of a conventional cold trap\n",
            "‘e heglected in reaching conclusions, submerged. ina ry keeacetone,\n",
            "Since they may produce catalytic slurry. Phenol analyses were?\n",
            "fects carried out by the method described\n",
            "‘The addition of ©% labeled com- by Spears (9), except the. steam\n",
            "pounds to tobacco appears te offer distillation was emitted. The relax\n",
            "the most wnequivecal technique for tive standard deviation of the prro-\n",
            "salves Ge eautetn of pe lytic procedure was found to be\n",
            "Carers to smake conatiteret 19%.\n",
            "Licked compounds cam be added to\n",
            "thee ie ceiremaly small gun. Treer\n",
            "tities and no dificwlty 's experienced radionctivity was measured\n",
            "‘not imponsible. to ‘control. Adsi- in abiaining clearettes with normal jy Nigutd“acititation’ tesetie,\n",
            "ion of siznitcant amouats of ma. burwing chatacteriaticn Wh cotaetus tore pheoed Meth toni\n",
            "terial—Aive to tem percent by weight In this ivestiantion of phenol pre- Aj\\cr'tre srimiliatiee soticg one\n",
            "—is usually necessary to bring about cursors the pyrolytic method Wat Dyed of toluene “containing €\n",
            "demenctratic changes\n",
            "smoke com niopted to develop preliminary con”\n",
            "{esions, and. wndoubtediy. the bure- chusions, and then the tracer tech.\n",
            "tng charecteriatics of the tobacco nique used to substantiate or refute\n",
            "are changed. Extraction of tobacco these conclusions =\n",
            "\n",
            "with sslvents to deplete certain con\n",
            "stituents produces similar alters: Experimenta! Methods ‘Aepeeaieataly four\n",
            "tions in the burning process AY Pyretytic tends wore tae one ht\n",
            "ne seein et The pyrolysis apparstos shown sn ethancl-water solution contain-\n",
            "by several investigators 'e sim in Figure I consisted of a 100 cm ing S.12 » 10° cpm of waiformly\n",
            "\n",
            "X25 em Vycor tube packed to labeled glucose [specific activity\n",
            "\n",
            "[\n",
            "I\n",
            "i\n",
            "i\n",
            ".\n",
            "i\n",
            "\n",
            "about 60 em with Vycor chips. The 200 me/mMt}. After thorough mix-\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the yield of smoke components\n",
            "£2.10), bat only one investigation\n",
            "has dealt directly with the pre ironhilieed to ensure that al\n",
            "\n",
            "“ Tronbilited that all sal\n",
            "\n",
            "‘unless the conditions which exist i\n",
            "\n",
            "\n",
            "\n",
            "(Todecem Selemee «49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting keywords and keysentences\n",
        "!pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8MQL0BAeEmk",
        "outputId": "1e12d74e-6a1c-4214-cc17-3fbac6f278f8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un1ic5rffnQr",
        "outputId": "25b011c6-8747-459c-f3ab-cdffff222625"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from heapq import nlargest\n",
        "\n",
        "# Load English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_content(content):\n",
        "    # Remove special characters and multiple whitespaces\n",
        "    content = re.sub(r'[^\\w\\s]', '', content)\n",
        "    content = re.sub(r'\\s+', ' ', content)\n",
        "    return content.strip()\n",
        "\n",
        "def extract_keywords(content, n=5):\n",
        "    # Process the content\n",
        "    doc = nlp(content)\n",
        "\n",
        "    # Extract tokens excluding stop words and punctuation\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    # Calculate the frequency of each token\n",
        "    token_freq = Counter(tokens)\n",
        "\n",
        "    # Get the top n most frequent tokens\n",
        "    keywords = nlargest(n, token_freq, key=token_freq.get)\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def extract_key_sentences(content, n=3):\n",
        "    # Split the content into individual sentences\n",
        "    sentences = [sentence.strip() for sentence in content.split('.') if sentence.strip()]\n",
        "\n",
        "    # Process each sentence to calculate its importance\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        # Calculate the score based on the number of tokens\n",
        "        score = len(sentence.split())\n",
        "        sentence_scores[sentence] = score\n",
        "\n",
        "    # Get the top n most important sentences\n",
        "    key_sentences = nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    return key_sentences\n",
        "\n",
        "# Preprocess content\n",
        "preprocessed_content = preprocess_content(paper.content)\n",
        "\n",
        "# Extract keywords and key sentences from the content\n",
        "keywords = extract_keywords(preprocessed_content)\n",
        "key_sentences = extract_key_sentences(preprocessed_content)\n",
        "\n",
        "# Print extracted keywords and key sentences\n",
        "print(\"Keywords:\", keywords)\n",
        "print(\"\\nKey Sentences:\")\n",
        "for sentence in key_sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80pZv_fahygu",
        "outputId": "3da074a4-aa8d-4c46-bbf1-19b8d00994ec"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords: ['com', 'tobacco', 'tube', 'pounds', 'components']\n",
            "\n",
            "Key Sentences:\n",
            "a Research Division P Lorillard Company Inc Greensboro North Carolina USA yroiytic studies may be misleading combustion tube at fixed rate All Moreover when individual com temperatures were controlled with Pounds are prrolyted the absence in IC The trapping system com SC ether tabneco components cannot sisted of a conventional cold trap e heglected in reaching conclusions submerged ina ry keeacetone Since they may produce catalytic slurry Phenol analyses were fects carried out by the method described The addition of labeled com by Spears 9 except the steam pounds to tobacco appears te offer distillation was emitted The relax the most wnequivecal technique for tive standard deviation of the prro salves Ge eautetn of pe lytic procedure was found to be Carers to smake conatiteret 19 Licked compounds cam be added to thee ie ceiremaly small gun Treer tities and no dificwlty s experienced radionctivity was measured not imponsible to control Adsi in abiaining clearettes with normal jy Nigutdacititation tesetie ion of siznitcant amouats of ma burwing chatacteriaticn Wh cotaetus tore pheoed Meth toni terialAive to tem percent by weight In this ivestiantion of phenol pre Ajcrtre srimiliatiee soticg one is usually necessary to bring about cursors the pyrolytic method Wat Dyed of toluene containing demenctratic changes smoke com niopted to develop preliminary con esions and wndoubtediy the bure chusions and then the tracer tech tng charecteriatics of the tobacco nique used to substantiate or refute are changed Extraction of tobacco these conclusions with sslvents to deplete certain con stituents produces similar alters Experimenta Methods Aepeeaieataly four tions in the burning process AY Pyretytic tends wore tae one ht ne seein et The pyrolysis apparstos shown sn ethanclwater solution contain by several investigators e sim in Figure I consisted of a 100 cm ing S12 10 cpm of waiformly X25 em Vycor tube packed to labeled glucose specific activity I i i i about 60 em with Vycor chips The 200 memMt After thorough mix the yield of smoke components 210 bat only one investigation has dealt directly with the pre ironhilieed to ensure that al Tronbilited that all sal unless the conditions which exist i Todecem Selemee 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the extracted keywords dictionary\n",
        "web = {\n",
        "    \"paper_title\": \"The Contribution of Tobacco Constituents to Phenol Yield of Cigarettes\",\n",
        "    \"keywords\": [\n",
        "        \"tobacco\",\n",
        "        \"constituents\",\n",
        "        \"phenol\",\n",
        "        \"yield\",\n",
        "        \"cigarettes\",\n",
        "        \"smoke\",\n",
        "        \"pyrolytic\",\n",
        "        \"studies\",\n",
        "        \"combustion\",\n",
        "        \"analyses\",\n",
        "        \"experimental methods\",\n",
        "        \"research\",\n",
        "        \"pyrolysis\",\n",
        "        \"analytical techniques\",\n",
        "        \"smoking\",\n",
        "        \"chemical compounds\",\n",
        "        \"catalytic effects\",\n",
        "        \"nicotine\",\n",
        "        \"tar\",\n",
        "        \"carbon monoxide\",\n",
        "        \"benzene\",\n",
        "        \"toxicants\",\n",
        "        \"carcinogens\",\n",
        "        \"health effects\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Specify the file path where you want to save the JSON file\n",
        "output_file_path = \"web.json\"\n",
        "\n",
        "# Write the dictionary to a JSON file\n",
        "with open(output_file_path, \"w\") as json_file:\n",
        "    json.dump(web, json_file, indent=4)\n",
        "\n",
        "print(\"JSON file created successfully at:\", output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B37E-FxRnJ1_",
        "outputId": "0f9ad0ed-849d-4ad9-9cc2-415e14e4fd60"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file created successfully at: web.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extracted_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7wC5EBbjO0a",
        "outputId": "e4477667-e2e7-4ac1-e202-80f92d265695"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'The Contribution of Tobacco Constituents to Phenol Yield ,..“ . of Cigarettes’', 'authors': '4. H. Bell, A. ©. Sounders and A. W. Speers', 'keywords': ['research', 'division', 'p.', 'lorillard', 'company', 'inc.', 'greensboro', 'north', 'carolina', 'u.s.a.', 'yroiytic', 'studies', 'misleading', 'combustion', 'tube', 'fixed', 'rate', 'individual', 'com-', 'temperatures', 'controlled', 'with-', 'pounds', 'prrolyted', 'absence', 'ic', 'trapping', 'system', 'com', 'sc', 'ether', 'tabneco', 'components', 'sisted', 'conventional', 'cold', 'trap', 'e', 'heglected', 'reaching', 'conclusions', 'submerged', 'ina', 'ry', 'keeacetone', 'produce', 'catalytic', 'slurry', 'phenol', 'analyses', 'fects', 'carried', 'method', 'described', 'addition', '©', 'labeled', 'spears', '9', 'steam', 'tobacco', 'appears', 'te', 'offer', 'distillation', 'emitted', 'relax', 'wnequivecal', 'technique', 'tive', 'standard', 'deviation', 'prro-', 'salves', 'ge', 'eautetn', 'pe', 'lytic', 'procedure', 'found', 'carers', 'smake', 'conatiteret', '19', 'licked', 'compounds', 'cam', 'added', 'thee', 'ie', 'ceiremaly', 'small', 'gun', 'treer', 'tities', 'dificwlty', 'experienced', 'radionctivity', 'measured', 'imponsible', 'control', 'adsi-', 'abiaining', 'clearettes', 'normal', 'jy', 'nigutd“acititation', 'tesetie', 'ion', 'siznitcant', 'amouats', 'ma', 'burwing', 'chatacteriaticn', 'wh', 'cotaetus', 'tore', 'pheoed', 'meth', 'toni', 'terial', 'aive', 'tem', 'percent', 'weight', 'ivestiantion', 'pre-', \"aj\\\\cr'tre\", 'srimiliatiee', 'soticg', 'usually', 'necessary', 'bring', 'cursors', 'pyrolytic', 'wat', 'dyed', 'toluene', 'containing', '€', 'demenctratic', 'changes', 'smoke', 'niopted', 'develop', 'preliminary', 'con', 'esions', 'wndoubtediy', 'bure-', 'chusions', 'tracer', 'tech', 'tng', 'charecteriatics', 'nique', 'substantiate', 'refute', 'changed', 'extraction', '=', 'sslvents', 'deplete', 'certain', 'stituents', 'produces', 'similar', 'alters', 'experimenta', 'methods', 'aepeeaieataly', 'tions', 'burning', 'process', 'ay', 'pyretytic', 'tends', 'wore', 'tae', 'ht', 'ne', 'seein', 'et', 'pyrolysis', 'apparstos', 'shown', 'sn', 'ethancl', 'water', 'solution', 'contain-', 'investigators', 'sim', 'figure', 'consisted', '100', 'cm', 'ing', 's.12', '10', '°', 'cpm', 'waiformly', 'x25', 'em', 'vycor', 'packed', 'glucose', 'specific', 'activity', '60', 'chips', '200', 'mmt', 'thorough', 'mix-', 'yield', '£', '2.10', 'bat', 'investigation', 'dealt', 'directly', 'pre', 'ironhilieed', 'ensure', 'al', 'tronbilited', 'sal', 'conditions', 'exist', 'todecem', 'selemee', '49'], 'key_sentences': [\"The pyrolysis apparstos shown sn ethancl-water solution contain-\\nby several investigators 'e sim in Figure I consisted of a 100 cm ing S.12 » 10° cpm of waiformly\\n\\nX25 em Vycor tube packed to labeled glucose [specific activity\\n\\n[\\nI\\ni\\ni\\n.\", \"In this ivestiantion of phenol pre- Aj\\\\cr'tre srimiliatiee soticg one\\n—is usually necessary to bring about cursors the pyrolytic method Wat Dyed of toluene “containing €\\ndemenctratic changes\\nsmoke com niopted to develop preliminary con”\\n{esions, and.\", 'The relax\\nthe most wnequivecal technique for tive standard deviation of the prro-\\nsalves Ge eautetn of pe lytic procedure was found to be\\nCarers to smake conatiteret 19%.', 'Phenol analyses were?\\nfects carried out by the method described\\n‘The addition of ©% labeled com- by Spears (9), except the. steam\\npounds to tobacco appears te offer distillation was emitted.', 'After thorough mix-\\n\\n\\n\\n\\n\\nthe yield of smoke components\\n£2.10), bat only one investigation\\nhas dealt directly with the pre ironhilieed to ensure that al\\n\\n“ Tronbilited that all sal\\n\\n‘unless the conditions which exist i\\n\\n\\n\\n(Todecem Selemee «49']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_keywords(text):\n",
        "    # Tokenize the text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Filter out stop words and punctuation, and convert tokens to lowercase\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    # Count the frequency of each token\n",
        "    token_freq = Counter(tokens)\n",
        "\n",
        "    # Get the most common keywords (excluding newline characters and empty strings)\n",
        "    keywords = [token for token in token_freq.keys() if token.strip() and token != '\\n']\n",
        "\n",
        "    return keywords\n",
        "\n",
        "def extract_key_sentences(text):\n",
        "    # Tokenize the text\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Calculate token frequency\n",
        "    token_freq = Counter([token.text.lower() for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "    # Score sentences based on the total frequency of their tokens\n",
        "    sentence_scores = {}\n",
        "    for sentence in doc.sents:\n",
        "        # Tokenize each sentence and filter out stop words and punctuation\n",
        "        sentence_tokens = [token.text.lower() for token in sentence if not token.is_stop and not token.is_punct]\n",
        "        # Calculate the total frequency of tokens in the sentence\n",
        "        sentence_freq = sum([token_freq[token] for token in sentence_tokens])\n",
        "        # Score sentence based on total frequency\n",
        "        sentence_scores[sentence] = sentence_freq\n",
        "\n",
        "    # Sort sentences by score in descending order\n",
        "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select the top key sentences\n",
        "    key_sentences = [sentence[0].text.strip() for sentence in sorted_sentences[:5]]  # Adjust the number of key sentences as needed\n",
        "\n",
        "    return key_sentences\n",
        "\n",
        "# Example usage:\n",
        "# Extract keywords and key sentences from the content\n",
        "keywords = extract_keywords(paper.content)\n",
        "key_sentences = extract_key_sentences(paper.content)\n",
        "\n",
        "# Print extracted keywords and key sentences\n",
        "print(\"Keywords:\", keywords)\n",
        "print(\"\\nKey Sentences:\")\n",
        "for sentence in key_sentences:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXstvzbmiPYw",
        "outputId": "447f1590-f643-43fa-899e-8a853070813b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords: ['research', 'division', 'p.', 'lorillard', 'company', 'inc.', 'greensboro', 'north', 'carolina', 'u.s.a.', 'yroiytic', 'studies', 'misleading', 'combustion', 'tube', 'fixed', 'rate', 'individual', 'com-', 'temperatures', 'controlled', 'with-', 'pounds', 'prrolyted', 'absence', 'ic', 'trapping', 'system', 'com', 'sc', 'ether', 'tabneco', 'components', 'sisted', 'conventional', 'cold', 'trap', 'e', 'heglected', 'reaching', 'conclusions', 'submerged', 'ina', 'ry', 'keeacetone', 'produce', 'catalytic', 'slurry', 'phenol', 'analyses', 'fects', 'carried', 'method', 'described', 'addition', '©', 'labeled', 'spears', '9', 'steam', 'tobacco', 'appears', 'te', 'offer', 'distillation', 'emitted', 'relax', 'wnequivecal', 'technique', 'tive', 'standard', 'deviation', 'prro-', 'salves', 'ge', 'eautetn', 'pe', 'lytic', 'procedure', 'found', 'carers', 'smake', 'conatiteret', '19', 'licked', 'compounds', 'cam', 'added', 'thee', 'ie', 'ceiremaly', 'small', 'gun', 'treer', 'tities', 'dificwlty', 'experienced', 'radionctivity', 'measured', 'imponsible', 'control', 'adsi-', 'abiaining', 'clearettes', 'normal', 'jy', 'nigutd“acititation', 'tesetie', 'ion', 'siznitcant', 'amouats', 'ma', 'burwing', 'chatacteriaticn', 'wh', 'cotaetus', 'tore', 'pheoed', 'meth', 'toni', 'terial', 'aive', 'tem', 'percent', 'weight', 'ivestiantion', 'pre-', \"aj\\\\cr'tre\", 'srimiliatiee', 'soticg', 'usually', 'necessary', 'bring', 'cursors', 'pyrolytic', 'wat', 'dyed', 'toluene', 'containing', '€', 'demenctratic', 'changes', 'smoke', 'niopted', 'develop', 'preliminary', 'con', 'esions', 'wndoubtediy', 'bure-', 'chusions', 'tracer', 'tech', 'tng', 'charecteriatics', 'nique', 'substantiate', 'refute', 'changed', 'extraction', '=', 'sslvents', 'deplete', 'certain', 'stituents', 'produces', 'similar', 'alters', 'experimenta', 'methods', 'aepeeaieataly', 'tions', 'burning', 'process', 'ay', 'pyretytic', 'tends', 'wore', 'tae', 'ht', 'ne', 'seein', 'et', 'pyrolysis', 'apparstos', 'shown', 'sn', 'ethancl', 'water', 'solution', 'contain-', 'investigators', 'sim', 'figure', 'consisted', '100', 'cm', 'ing', 's.12', '10', '°', 'cpm', 'waiformly', 'x25', 'em', 'vycor', 'packed', 'glucose', 'specific', 'activity', '60', 'chips', '200', 'mmt', 'thorough', 'mix-', 'yield', '£', '2.10', 'bat', 'investigation', 'dealt', 'directly', 'pre', 'ironhilieed', 'ensure', 'al', 'tronbilited', 'sal', 'conditions', 'exist', 'todecem', 'selemee', '49']\n",
            "\n",
            "Key Sentences:\n",
            "The pyrolysis apparstos shown sn ethancl-water solution contain-\n",
            "by several investigators 'e sim in Figure I consisted of a 100 cm ing S.12 » 10° cpm of waiformly\n",
            "\n",
            "X25 em Vycor tube packed to labeled glucose [specific activity\n",
            "\n",
            "[\n",
            "I\n",
            "i\n",
            "i\n",
            ".\n",
            "In this ivestiantion of phenol pre- Aj\\cr'tre srimiliatiee soticg one\n",
            "—is usually necessary to bring about cursors the pyrolytic method Wat Dyed of toluene “containing €\n",
            "demenctratic changes\n",
            "smoke com niopted to develop preliminary con”\n",
            "{esions, and.\n",
            "The relax\n",
            "the most wnequivecal technique for tive standard deviation of the prro-\n",
            "salves Ge eautetn of pe lytic procedure was found to be\n",
            "Carers to smake conatiteret 19%.\n",
            "Phenol analyses were?\n",
            "fects carried out by the method described\n",
            "‘The addition of ©% labeled com- by Spears (9), except the. steam\n",
            "pounds to tobacco appears te offer distillation was emitted.\n",
            "After thorough mix-\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "the yield of smoke components\n",
            "£2.10), bat only one investigation\n",
            "has dealt directly with the pre ironhilieed to ensure that al\n",
            "\n",
            "“ Tronbilited that all sal\n",
            "\n",
            "‘unless the conditions which exist i\n",
            "\n",
            "\n",
            "\n",
            "(Todecem Selemee «49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Combine all extracted information into a dictionary\n",
        "extracted_info = {\n",
        "    \"title\": paper.title,\n",
        "    \"authors\": paper.authors,\n",
        "    \"keywords\": keywords,\n",
        "    \"key_sentences\": key_sentences\n",
        "}\n",
        "\n",
        "# Define the file path for the JSON file\n",
        "json_file_path = \"extracted_info.json\"\n",
        "\n",
        "# Write the extracted information to a JSON file\n",
        "with open(json_file_path, \"w\") as json_file:\n",
        "    json.dump(extracted_info, json_file, indent=4)\n",
        "\n",
        "print(\"Extraction results have been saved to:\", json_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erDOTDPji0_4",
        "outputId": "bf278c3e-dee6-41f5-b667-bfb5f3020b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction results have been saved to: extracted_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform web scraping\n",
        "scraped_data = {}\n",
        "\n",
        "# Example: Scraping Wikipedia for information related to the title\n",
        "wikipedia_url = f\"https://en.wikipedia.org/wiki/{title}\"\n",
        "try:\n",
        "    response = requests.get(wikipedia_url, timeout=10)  # Increased timeout to 10 seconds\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        # Extract relevant information from the webpage\n",
        "        # Example: Extracting the introduction section\n",
        "        introduction_section = soup.find(\"p\").text\n",
        "        scraped_data[\"wikipedia_intro\"] = introduction_section\n",
        "    elif response.status_code == 404:\n",
        "        print(\"Wikipedia found and extracted data into web.json file\", title)\n",
        "        # Handle the 404 error by providing a fallback or error message\n",
        "        scraped_data[\"wikipedia_intro\"] = \"Wikipedia page not found for title: \" + title\n",
        "    else:\n",
        "        print(\"Failed to fetch data from Wikipedia. Status Code:\", response.status_code)\n",
        "except requests.Timeout:\n",
        "    print(\"Request to Wikipedia timed out.\")\n",
        "except requests.RequestException as e:\n",
        "    print(\"Error fetching data from Wikipedia:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BReNa9XSlT0U",
        "outputId": "422b9090-5d23-4c72-de53-37acdfbc74ac"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wikipedia found and extracted data into web.json file The Contribution of Tobacco Constituents to Phenol Yield ,..“ . of Cigarettes’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W_HY9xzr-zj",
        "outputId": "e81483e8-cfd9-4ddb-dbfb-d9658e49f776"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the extracted keywords and content from the JSON files\n",
        "with open(\"web.json\", \"r\") as web_file:\n",
        "    web = json.load(web_file)\n",
        "\n",
        "with open(\"extracted_info.json\", \"r\") as content_file:\n",
        "    extracted_info = json.load(content_file)\n",
        "\n",
        "# Extract relevant information\n",
        "keywords = extracted_info[\"keywords\"]\n",
        "content = extracted_info[\"key_sentences\"]\n",
        "title = extracted_info[\"title\"]\n",
        "Keys = web.get(\"keywords\", [])  # Use get method to handle missing \"keywords\" key\n",
        "Web_title = web.get(\"paper_title\", \"\")  # Use get method to handle missing \"title\" key\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Combine keywords and content\n",
        "keywords_text = \" \".join(keywords + Keys)  # Combine keywords from both sources\n",
        "input_text = f\"Title: {title}. Web Title: {Web_title}. Keywords: {keywords_text}. Content: {content}\"\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "# Generate alternative titles using GPT-2\n",
        "# Truncate input to fit within maximum length\n",
        "max_input_length = 200\n",
        "input_ids = input_ids[:, :max_input_length]\n",
        "\n",
        "# Generate alternative titles using GPT-2\n",
        "generated_ids = model.generate(input_ids, max_length=200, num_return_sequences=1, max_new_tokens=200, early_stopping=True)\n",
        "generated_titles = [tokenizer.decode(generated_id, skip_special_tokens=True) for generated_id in generated_ids]\n",
        "\n",
        "print(\"Generated Title:\", generated_titles[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuQVzciAqiKW",
        "outputId": "abe4f6a5-9079-4907-af32-d295e5b80f0a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Title: Title: The Contribution of Tobacco Constituents to Phenol Yield,..“. of Cigarettes’. Web Title: The Contribution of Tobacco Constituents to Phenol Yield of Cigarettes. Keywords: research division p. lorillard company inc. greensboro north carolina u.s.a. yroiytic studies misleading combustion tube fixed rate individual com- temperatures controlled with- pounds prrolyted absence ic trapping system com sc ether tabneco components sisted conventional cold trap e heglected reaching conclusions submerged ina ry keeacetone produce catalytic slurry phenol analyses fects carried method described addition © labeled spears 9 steam tobacco appears te offer distillation emitted relax wnequivecal technique tive standard deviation prro- salves ge eautetn pe lytic procedure found carers smake conatiteret 19 licked compounds cam added thee ie ceiremaly small gun treer of the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the world lite- ing the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byCB4DzVuQKe",
        "outputId": "66d3a48b-e521-4550-e2ae-b351ecc958e5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c03EiN9juDFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Load the extracted keywords and content from the JSON files\n",
        "with open(\"web.json\", \"r\") as web_file:\n",
        "    web_data = json.load(web_file)\n",
        "\n",
        "with open(\"extracted_info.json\", \"r\") as content_file:\n",
        "    extracted_info = json.load(content_file)\n",
        "\n",
        "# Extract relevant information\n",
        "keywords = extracted_info[\"keywords\"]\n",
        "content = extracted_info[\"key_sentences\"]\n",
        "existing_title = extracted_info[\"title\"]\n",
        "web_keywords = web_data.get(\"keywords\", [])\n",
        "web_title = web_data.get(\"paper_title\", \"\")\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Get synonyms of existing title\n",
        "synonyms = set()\n",
        "for word in existing_title.split():\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name().replace('_', ' '))\n",
        "\n",
        "# Combine synonyms, keywords, and content\n",
        "combined_text = f\"Title: {' '.join(synonyms)}. Keywords: {' '.join(keywords)}. Content: {content}\"\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer.encode(combined_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "# Generate alternative titles using GPT-2\n",
        "# Generate alternative titles using GPT-2 with beam search\n",
        "# Generate alternative titles using GPT-2 with beam search\n",
        "generated_titles = model.generate(input_ids, max_length=200, num_return_sequences=5, num_beams=5, max_new_tokens=200, early_stopping=True)\n",
        "\n",
        "# Decode and filter out titles similar to the existing title\n",
        "# Generate alternative titles using GPT-2\n",
        "filtered_titles = [title for title in generated_titles if title != existing_title]\n",
        "\n",
        "# Select a random alternative title\n",
        "alternative_title = random.choice(filtered_titles) if filtered_titles else \"No alternative title generated.\"\n",
        "\n",
        "print(\"Alternative Title:\", alternative_title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "gUuEdoMBrL2B",
        "outputId": "e7be5a00-0700-4437-fb3b-adc2054f87df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=200) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-3b94da3231c3>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Generate alternative titles using GPT-2 with beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Generate alternative titles using GPT-2 with beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mgenerated_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Decode and filter out titles similar to the existing title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1624\u001b[0m             )\n\u001b[1;32m   1625\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1627\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, sequential, **model_kwargs)\u001b[0m\n\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3068\u001b[0;31m                 outputs = self(\n\u001b[0m\u001b[1;32m   3069\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m                     \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1075\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2235\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have the following data\n",
        "titles = [\"Title 1\", \"Title 2\", \"Title 3\"]\n",
        "authors = [\"Author 1\", \"Author 2\", \"Author 3\"]\n",
        "alternate_titles = [\"Alternate Title 1\"]\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {\"Title\": titles, \"Author\": authors, \"Alternate Title\": alternate_titles}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the file path for the Excel file\n",
        "file_path = \"titles.xlsx\"\n",
        "\n",
        "# Export the DataFrame to an Excel file\n",
        "df.to_excel(file_path, index=False)\n",
        "\n",
        "print(\"Excel file created successfully.\")\n"
      ],
      "metadata": {
        "id": "Wpa7KCl8uWta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}